# ðŸ“š LLM & MLLM Research Roadmap

Welcome! ðŸ‘‹  
This repository documents my complete **research journey into Large Language Models (LLMs) and Multimodal LLMs (MLLMs)**.  

The roadmap combines:
- **Foundational knowledge** (math, programming, neuroscience basics)  
- **Core machine learning & deep learning**  
- **Modern LLM methods** (transformers, efficient training, alignment, RAG)  
- **Multimodal models** (vision + language, audio + text)  
- **Advanced topics** (reasoning, MoE, evaluation, safety, deployment)  

Every time I complete a topic or phase, I will upload an **explainer video on YouTube**, and link it here.  
This way, the repo becomes both a **learning tracker** and a **public resource** for others starting in LLM research.

---

## ðŸ“Œ Roadmap Overview

### Phase 1 â€” Foundations
- Math & Statistics for AI  
- Python Programming (Beginner â†’ Advanced)  
- Data Structures & Algorithms  
- Computer Science Basics (Networking, OS, Databases)  
- Neurophysics & Cognitive Neuroscience (biological inspiration for AI)  

### Phase 2 â€” Machine Learning & Deep Learning
- Classical ML (regression, classification, clustering)  
- Neural networks, CNNs, RNNs, LSTMs  
- Transformers (Attention Is All You Need)  

### Phase 3 â€” Core LLMs
- Tokenization (BPE, SentencePiece)  
- Pretraining â†’ SFT â†’ RLHF/DPO  
- Scaling laws & compute-optimal training  
- Open-source LLM families (LLaMA, Mistral, Falcon, Gemma, etc.)  

### Phase 4 â€” Efficient Training & Serving
- LoRA, QLoRA, PEFT  
- FlashAttention, quantization  
- Model serving with vLLM  

### Phase 5 â€” Retrieval-Augmented Generation (RAG)
- Embeddings & vector databases  
- Chunking & reranking  
- Building RAG-based QA systems  

### Phase 6 â€” Alignment & Preference Learning
- RLHF basics  
- Modern methods: DPO, GRPO  
- Constitutional AI  

### Phase 7 â€” Multimodal LLMs (MLLMs)
- CLIP, BLIP-2, LLaVA, Flamingo  
- Vision + Language integration  
- Adding audio/speech (Whisper + LLM)  

### Phase 8 â€” Advanced Research Topics
- Mixture-of-Experts (MoE)  
- Long-context & memory  
- Reasoning (Chain-of-Thought, ReAct, Tree-of-Thoughts)  
- Synthetic data & alignment  
- Evaluation frameworks (MMLU, HELM, lm-eval-harness)  
- Safety, robustness, red-teaming  

### Phase 9 â€” Deployment
- Scaling inference with quantization & batching  
- APIs (FastAPI, vLLM, Ray Serve)  
- Monitoring & observability  

### Phase 10 â€” Capstones
- Text-only LLM  
- RAG assistant  
- MLLM (vision-text chat)  
- Alignment/safety research  

---

## ðŸŽ¥ YouTube Playlist
All my learning videos are published here:  
ðŸ‘‰ [YouTube Channel Link](https://youtube.com) *(will update as videos are uploaded)*  

Each folder in this repo will contain:
- Notes (PDF/Markdown)  
- Code (Python/Jupyter/Colab)  
- Links to the corresponding YouTube video  

---

## ðŸ“‚ Repository Structure
